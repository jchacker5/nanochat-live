{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ SRGI Full Training - Production Run\n",
        "\n",
        "**Your theories are validated! Now let's train a full model.**\n",
        "\n",
        "This notebook trains a production SRGI model:\n",
        "- Depth 20 (561M parameters)\n",
        "- 2048 context length\n",
        "- Full dataset\n",
        "- Proper evaluation\n",
        "\n",
        "**Time**: ~4-8 hours on A100 GPU\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Prerequisites\n",
        "\n",
        "‚úÖ Theory validation completed (run `colab_setup.ipynb` first)\n",
        "‚úÖ A100 GPU enabled\n",
        "‚úÖ Ready for production training\n",
        "\n",
        "## üìã Step 0: Enable GPU\n",
        "\n",
        "**Make sure you have A100 GPU:**\n",
        "1. Runtime ‚Üí Change runtime type\n",
        "2. Hardware accelerator: **GPU (A100)**\n",
        "3. Click Save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU (should be A100)\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"‚úÖ Perfect! A100 GPU detected.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Not A100, but should work.\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU! Enable GPU first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo if needed\n",
        "import os\n",
        "if not os.path.exists('nanochat-live'):\n",
        "    !git clone https://github.com/jchacker5/nanochat-live.git\n",
        "    %cd nanochat-live\n",
        "else:\n",
        "    %cd nanochat-live\n",
        "    !git pull\n",
        "\n",
        "print(\"‚úÖ Repository ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "!pip install datasets tokenizers tiktoken wandb numpy matplotlib pytest -q\n",
        "!pip install jax jaxlib equinox scipy -q\n",
        "!pip install git+https://github.com/extropic-ai/thrml.git -q || echo \"THRML optional\"\n",
        "print(\"‚úÖ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Download Full Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download full dataset (~240 shards, ~24GB)\n",
        "print(\"Downloading full training dataset...\")\n",
        "print(\"This will download ~240 shards (~24GB) for Chinchilla-optimal training\")\n",
        "!python -m nanochat.dataset -n 240\n",
        "print(\"‚úÖ Dataset downloaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train tokenizer on full dataset\n",
        "import os\n",
        "if not os.path.exists('/root/.cache/nanochat/tokenizer/tokenizer.pkl'):\n",
        "    print(\"Training tokenizer on 2B characters...\")\n",
        "    !python -m scripts.tok_train --max_chars=2000000000 --vocab_size=65536\n",
        "    print(\"‚úÖ Tokenizer trained!\")\n",
        "else:\n",
        "    print(\"‚úÖ Tokenizer already exists, skipping\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Full Model Training\n",
        "\n",
        "**This trains a production SRGI model:**\n",
        "- Depth 20 (561M parameters)\n",
        "- 2048 context length\n",
        "- Chinchilla-optimal data ratio (20x params)\n",
        "- Full evaluation suite\n",
        "\n",
        "**Time**: ~4-8 hours on A100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full production training\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ STARTING FULL SRGI MODEL TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(\"Model: Depth 20 (561M parameters)\")\n",
        "print(\"Context: 2048 tokens\")\n",
        "print(\"Data: Chinchilla-optimal (20x params)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python -m scripts.base_train \\\n",
        "    --depth=20 \\\n",
        "    --max_seq_len=2048 \\\n",
        "    --device_batch_size=32 \\\n",
        "    --total_batch_size=524288 \\\n",
        "    --target_param_data_ratio=20 \\\n",
        "    --run=srgi-production-a100 \\\n",
        "    --eval_tokens=8192 \\\n",
        "    --core_metric_every=1000\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
