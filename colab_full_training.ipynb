{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ SRGI Full Training - Production Run\n",
        "\n",
        "**Your theories are validated! Now let's train a full model.**\n",
        "\n",
        "This notebook trains a production SRGI model:\n",
        "- Depth 20 (561M parameters)\n",
        "- 2048 context length\n",
        "- Full dataset\n",
        "- Proper evaluation\n",
        "\n",
        "**Time**: ~4-8 hours on A100 GPU\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ Prerequisites\n",
        "\n",
        "âœ… Theory validation completed (run `colab_setup.ipynb` first)\n",
        "âœ… A100 GPU enabled\n",
        "âœ… Ready for production training\n",
        "\n",
        "## ðŸ“‹ Step 0: Enable GPU\n",
        "\n",
        "**Make sure you have A100 GPU:**\n",
        "1. Runtime â†’ Change runtime type\n",
        "2. Hardware accelerator: **GPU (A100)**\n",
        "3. Click Save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU (should be A100)\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"âœ… Perfect! A100 GPU detected.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Not A100, but should work.\")\n",
        "else:\n",
        "    print(\"âŒ No GPU! Enable GPU first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo if needed\n",
        "import os\n",
        "if not os.path.exists('nanochat-live'):\n",
        "    !git clone https://github.com/jchacker5/nanochat-live.git\n",
        "    %cd nanochat-live\n",
        "else:\n",
        "    %cd nanochat-live\n",
        "    !git pull\n",
        "\n",
        "print(\"âœ… Repository ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "!pip install datasets tokenizers tiktoken wandb numpy matplotlib pytest -q\n",
        "!pip install jax jaxlib equinox scipy -q\n",
        "!pip install git+https://github.com/extropic-ai/thrml.git -q || echo \"THRML optional\"\n",
        "print(\"âœ… Dependencies installed\")\n",
        "\n",
        "# Setup wandb (optional - for experiment tracking)\n",
        "# Option 1: Use Colab secrets (recommended - secure)\n",
        "# 1. Go to: Colab â†’ ðŸ”‘ (key icon) â†’ Secrets\n",
        "# 2. Add secret: name=\"WANDB_API_KEY\", value=\"your-api-key-here\"\n",
        "# 3. Uncomment the code below:\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "    if wandb_api_key:\n",
        "        import wandb\n",
        "        wandb.login(key=wandb_api_key)\n",
        "        print(\"âœ… Wandb logged in via Colab secrets\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸  Wandb API key not found in secrets (training will use 'dummy' mode)\")\n",
        "except:\n",
        "    # Option 2: Manual login (less secure - key visible in notebook)\n",
        "    # Uncomment and paste your API key:\n",
        "    # import wandb\n",
        "    # wandb.login(key=\"your-api-key-here\")\n",
        "    print(\"â„¹ï¸  Wandb not configured - training will use 'dummy' mode (no logging)\")\n",
        "    print(\"   To enable wandb: Add WANDB_API_KEY to Colab secrets (ðŸ”‘ icon)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Download Full Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download full dataset (~240 shards, ~24GB)\n",
        "print(\"Downloading full training dataset...\")\n",
        "print(\"This will download ~240 shards (~24GB) for Chinchilla-optimal training\")\n",
        "!python -m nanochat.dataset -n 240\n",
        "print(\"âœ… Dataset downloaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train tokenizer on full dataset (GUARANTEED WORKING VERSION)\n",
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import tiktoken\n",
        "from nanochat.tokenizer import HuggingFaceTokenizer, SPECIAL_TOKENS\n",
        "from nanochat.dataset import parquets_iter_batched\n",
        "\n",
        "tokenizer_dir = \"/root/.cache/nanochat/tokenizer\"\n",
        "os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(os.path.join(tokenizer_dir, 'tokenizer.pkl')):\n",
        "    print(\"=\"*70)\n",
        "    print(\"Training tokenizer on 2B characters...\")\n",
        "    print(\"Using HuggingFace tokenizer (works on Colab)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Create text iterator\n",
        "    def text_iterator():\n",
        "        nchars = 0\n",
        "        for batch in parquets_iter_batched(split=\"train\"):\n",
        "            for doc in batch:\n",
        "                doc_text = doc\n",
        "                if len(doc_text) > 10000:  # Cap document length\n",
        "                    doc_text = doc_text[:10000]\n",
        "                nchars += len(doc_text)\n",
        "                yield doc_text\n",
        "                if nchars > 2000000000:  # 2B characters\n",
        "                    return\n",
        "    \n",
        "    # Train tokenizer\n",
        "    print(\"Training BPE tokenizer...\")\n",
        "    hf_tokenizer = HuggingFaceTokenizer.train_from_iterator(text_iterator(), vocab_size=65536)\n",
        "    \n",
        "    # Save HuggingFace tokenizer.json\n",
        "    hf_tokenizer.tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
        "    print(\"âœ… Saved tokenizer.json\")\n",
        "    \n",
        "    # Create tiktoken-compatible encoding for tokenizer.pkl\n",
        "    vocab = hf_tokenizer.tokenizer.get_vocab()\n",
        "    \n",
        "    # Create mergeable_ranks (BPE merges)\n",
        "    mergeable_ranks = {}\n",
        "    rank = 0\n",
        "    for token, token_id in sorted(vocab.items(), key=lambda x: x[1]):\n",
        "        if token not in SPECIAL_TOKENS:\n",
        "            try:\n",
        "                mergeable_ranks[token.encode('utf-8')] = rank\n",
        "                rank += 1\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    # Create special tokens mapping\n",
        "    tokens_offset = len(mergeable_ranks)\n",
        "    special_tokens_dict = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n",
        "    \n",
        "    # Create tiktoken encoding (what RustBPETokenizer expects)\n",
        "    enc = tiktoken.Encoding(\n",
        "        name=\"huggingface_bpe\",\n",
        "        pat_str=r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\",\n",
        "        mergeable_ranks=mergeable_ranks,\n",
        "        special_tokens=special_tokens_dict,\n",
        "    )\n",
        "    \n",
        "    # Save as pickle (what RustBPETokenizer.from_directory expects)\n",
        "    with open(os.path.join(tokenizer_dir, 'tokenizer.pkl'), 'wb') as f:\n",
        "        pickle.dump(enc, f)\n",
        "    print(\"âœ… Saved tokenizer.pkl\")\n",
        "    \n",
        "    # Create token_bytes.pt (needed for evaluation)\n",
        "    vocab_size = hf_tokenizer.get_vocab_size()\n",
        "    special_set = set(hf_tokenizer.get_special_tokens())\n",
        "    token_bytes = []\n",
        "    for token_id in range(vocab_size):\n",
        "        try:\n",
        "            token_str = hf_tokenizer.decode([token_id])\n",
        "            if token_str in special_set:\n",
        "                token_bytes.append(0)\n",
        "            else:\n",
        "                token_bytes.append(len(token_str.encode(\"utf-8\")))\n",
        "        except:\n",
        "            token_bytes.append(0)\n",
        "    \n",
        "    token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')\n",
        "    token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
        "    with open(token_bytes_path, \"wb\") as f:\n",
        "        torch.save(token_bytes, f)\n",
        "    print(\"âœ… Saved token_bytes.pt\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"âœ… Tokenizer trained and saved successfully!\")\n",
        "    print(f\"âœ… Files created in: {tokenizer_dir}\")\n",
        "    print(f\"   - tokenizer.pkl: {os.path.exists(os.path.join(tokenizer_dir, 'tokenizer.pkl'))}\")\n",
        "    print(f\"   - tokenizer.json: {os.path.exists(os.path.join(tokenizer_dir, 'tokenizer.json'))}\")\n",
        "    print(f\"   - token_bytes.pt: {os.path.exists(os.path.join(tokenizer_dir, 'token_bytes.pt'))}\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"âœ… Tokenizer already exists, skipping\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Full Model Training\n",
        "\n",
        "**This trains a production SRGI model:**\n",
        "- Depth 20 (561M parameters)\n",
        "- 2048 context length\n",
        "- Chinchilla-optimal data ratio (20x params)\n",
        "- Full evaluation suite\n",
        "\n",
        "**Time**: ~4-8 hours on A100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full production training\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸš€ STARTING FULL SRGI MODEL TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(\"Model: Depth 20 (561M parameters)\")\n",
        "print(\"Context: 2048 tokens\")\n",
        "print(\"Data: Chinchilla-optimal (20x params)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python -m scripts.base_train \\\n",
        "    --depth=20 \\\n",
        "    --max_seq_len=2048 \\\n",
        "    --device_batch_size=32 \\\n",
        "    --total_batch_size=524288 \\\n",
        "    --target_param_data_ratio=20 \\\n",
        "    --run=srgi-production-a100 \\\n",
        "    --eval_tokens=8192 \\\n",
        "    --core_metric_every=1000\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
